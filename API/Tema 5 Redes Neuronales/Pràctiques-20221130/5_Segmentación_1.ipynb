{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5_segmentation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"bHWUgZz0xS6J"},"source":["##peculiaridad de la arquitectura de segmentación\n","import tensorflow.keras.backend as K\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose, Activation, BatchNormalization, UpSampling2D, Add\n","from tensorflow.keras.optimizers import Adam\n","\n","\n","#otras funciones de perdidas que NO sean las \"típicas\" error cuadrático, ....\n","#coeficiente dice.  Mide la intersección entre las máscaras.\n","def dice_coef(y_true, y_pred):     #función del coeficiente dice. (entre 0 y 1, cuanto más cercano a 1 es mejor)\n","    smooth = 1.\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n","\n","\n","def dice_coef_loss(y_true, y_pred):   #pérdidas (ponerle coeficiente negativo)\n","    return -dice_coef(y_true, y_pred)\n","\n","\n","def get_unet2D(img_rows, img_cols, img_ch):   #arquitectura de unet2D\n","    # Encoding phase\n","    inputs = Input((img_rows, img_cols, img_ch))   #imagen de entrada\n","    conv1 = Conv2D(32, (3, 3), padding='same')(inputs)  #capa convolucional\n","    conv1 = BatchNormalization()(conv1)    #aplicamos normalización a la salida del filtro convolucional.  ((Menos la media entre la desviación estándar))\n","    conv1 = Activation('relu')(conv1)  #separo la de activación. la hago después de normalizar.\n","    conv1 = Conv2D(32, (3, 3), padding='same')(conv1)\n","    conv1 = BatchNormalization()(conv1)\n","    conv1 = Activation('relu')(conv1)\n","    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n","\n","    conv2 = Conv2D(64, (3, 3), padding='same')(pool1)\n","    conv2 = BatchNormalization()(conv2)\n","    conv2 = Activation('relu')(conv2)\n","    conv2 = Conv2D(64, (3, 3), padding='same')(conv2)\n","    conv2 = BatchNormalization()(conv2)\n","    conv2 = Activation('relu')(conv2)\n","    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","\n","    conv3 = Conv2D(128, (3, 3), padding='same')(pool2)\n","    conv3 = BatchNormalization()(conv3)\n","    conv3 = Activation('relu')(conv3)\n","    conv3 = Conv2D(128, (3, 3), padding='same')(conv3)\n","    conv3 = BatchNormalization()(conv3)\n","    conv3 = Activation('relu')(conv3)\n","    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","\n","    conv4 = Conv2D(256, (3, 3), padding='same')(pool3)\n","    conv4 = BatchNormalization()(conv4)\n","    conv4 = Activation('relu')(conv4)\n","    conv4 = Conv2D(256, (3, 3), padding='same')(conv4)\n","    conv4 = BatchNormalization()(conv4)\n","    conv4 = Activation('relu')(conv4)\n","    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n","\n","    # Decoding phase            #NO tenemos \"Dense\"\n","    conv5 = Conv2D(512, (3, 3), padding='same')(pool4)\n","    conv5 = BatchNormalization()(conv5)\n","    conv5 = Activation('relu')(conv5)\n","    conv5 = Conv2D(512, (3, 3), padding='same')(conv5)\n","    conv5 = BatchNormalization()(conv5)\n","    conv5 = Activation('relu')(conv5)\n","\n","    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)    #\"conexiones puente\"\n","    conv6 = Conv2D(256, (3, 3), padding='same')(up6)\n","    conv6 = BatchNormalization()(conv6)\n","    conv6 = Activation('relu')(conv6)\n","    conv6 = Conv2D(256, (3, 3), padding='same')(conv6)\n","    conv6 = BatchNormalization()(conv6)\n","    conv6 = Activation('relu')(conv6)\n","\n","    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n","    conv7 = Conv2D(128, (3, 3), padding='same')(up7)\n","    conv7 = BatchNormalization()(conv7)\n","    conv7 = Activation('relu')(conv7)\n","    conv7 = Conv2D(128, (3, 3), padding='same')(conv7)\n","    conv7 = BatchNormalization()(conv7)\n","    conv7 = Activation('relu')(conv7)\n","\n","    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n","    conv8 = Conv2D(64, (3, 3), padding='same')(up8)\n","    conv8 = BatchNormalization()(conv8)\n","    conv8 = Activation('relu')(conv8)\n","    conv8 = Conv2D(64, (3, 3), padding='same')(conv8)\n","    conv8 = BatchNormalization()(conv8)\n","    conv8 = Activation('relu')(conv8)\n","\n","    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n","    conv9 = Conv2D(32, (3, 3), padding='same')(up9)\n","    conv9 = BatchNormalization()(conv9)\n","    conv9 = Activation('relu')(conv9)\n","    conv9 = Conv2D(32, (3, 3), padding='same')(conv9)\n","    conv9 = BatchNormalization()(conv9)\n","    conv9 = Activation('relu')(conv9)\n","\n","    # Output\n","    output = Activation('sigmoid')(conv9)    #la función de activación dependerá si sólo tengo dos clases o más de una (softmax)\n","\n","    # Compile model with inputs and outputs\n","    model = Model(inputs=[inputs], outputs=[output])\n","\n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UE6mgzBL8qYA"},"source":["from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","\n","def train():\n","\n","  model = get_unet2D(32, 32, 3)\n","\n","  model.compile(optimizer=\"adam\", loss=dice_coef_loss, metrics=[dice_coef])    #las funciones las he definido yo, añadimos otras métricas\n","\n","  model_checkpoint = ModelCheckpoint(filepath='./models/unet.h5',\n","                                     monitor='val_loss',   #donde me interesa que sea bueno (pérdidas en validación)\n","                                     save_best_only=True)  #me guardo sólo el mejor.\n","  \n","  rlrp = ReduceLROnPlateau(monitor='val_loss',      #reducimos la función de pérdidas según vaya mi entrenamiento.\n","                           factor=0.1,              #al principio tasa de aprendizaje grande y va reduciendo.\n","                           patience=5,              #5 iteraciones igual sin cambio.\n","                           min_delta=1e-7,          #NO baja de este valor.\n","                           verbose=1)\n","  \n","  earlystopping = EarlyStopping(monitor=\"val_loss\",\n","                                mode=\"min\", patience=5,    #5 iteraciones sin mejorar\n","                                restore_best_weights=True,\n","                                verbose=1)\n","  \n","  callbacks = [model_checkpoint, rlrp, earlystopping]\n","\n","  H = model.fit(data_generator_train,\n","                steps_per_epoch=X_train.shape[0] // batch_size,\n","                epochs=20,\n","                verbose=1,\n","                callbacks=callbacks,\n","                validation_data=(X_val, y_val))\n"],"execution_count":null,"outputs":[]}]}