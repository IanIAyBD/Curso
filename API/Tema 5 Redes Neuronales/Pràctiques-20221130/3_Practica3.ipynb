{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Practica3.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyO7VX6o1JZoWf1eVh1ewfLf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"plfuYxghvJUu"},"source":["# Práctica 3: Fundamentos CNN"]},{"cell_type":"markdown","metadata":{"id":"AKLRXQB3vX0W"},"source":["En primer lugar vamos a tratar de resolver el problema de clasificación con la base de datos CIFAR 10 que no pudimos solventar en la práctica anterior empleando redes neuronales totalmente conectadas.\n","\n","Para el desarrollo de esta práctica vamos a activar el uso de GPU. Para ellos accede a: \n","Entorno de ejecución -> Cambiar tipo de entorno de ejecución -> Acelerador por hardware -> GPU -> Guardar"]},{"cell_type":"code","metadata":{"id":"pqFwxEm-vGK0"},"source":["# Importamos la base de datos\n","from tensorflow.keras.datasets import cifar10\n","\n","(X_train, y_train), (X_testval, y_testval) = cifar10.load_data()\n","\n","print(X_train.shape, y_train.shape)\n","print(X_testval.shape, y_testval.shape)\n","\n","print('Valor mínimo imágenes: ', X_train.min())\n","print('Valor máximo imágenes: ', X_train.max())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4MDULjLGa6BC"},"source":["import matplotlib.pyplot as plt\n","\n","# Vector de los nombres de las clases definidas en CIFAR\n","classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', \n","           'horse', \"ship\", \"truck\"]\n","\n","# Mostramos 9 imágenes\n","plt.rcParams['figure.figsize'] = (10, 10)\n","for i in range(9):\n","  plt.subplot(3, 3, i+1)\n","  plt.imshow(X_train[i])\n","  plt.title(f'{classes[int(y_train[i])]}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XTJKGqNX0WK9","executionInfo":{"status":"ok","timestamp":1621544151701,"user_tz":-120,"elapsed":688,"user":{"displayName":"Ana Jiménez Pastor","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgrLiw2d84vMG3Q3k5fXBnRwT77yp2o-XE46Ej8SA=s64","userId":"00856032343269336878"}}},"source":["# Convertimos rango imágenes a 0-1\n","X_train = X_train.astype('float32')\n","X_testval = X_testval.astype('float32')\n","X_train /= 255\n","X_testval /= 255"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nrf-nkFi0aAW"},"source":["from sklearn.preprocessing import OneHotEncoder\n","# Convertimos etiquetas a codificación one-hot\n","from tensorflow.keras.utils import to_categorical\n","num_clases = len(np.unique(y_train))\n","y_train_cod = to_categorical(y_train, num_clases)\n","y_testval_cod = to_categorical(y_testval, num_clases)\n","print(\"Tamaño etiquetas entrenamiento: \", y_train_cod.shape)\n","print(\"Tamaño etiquetas validación/test: \", y_testval_cod.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_JQlq1ED0j88"},"source":["# Dividimos conjunto de datos de validación/test en 2 subconjuntos\n","from sklearn.model_selection import train_test_split\n","X_val, X_test, y_val, y_test = train_test_split(X_testval, y_testval_cod,\n","                                                test_size=0.5)\n","\n","print(\"Muestras validación: \", X_val.shape)\n","print(\"Salida validación: \", y_val.shape)\n","print(\"Muestras test: \", X_test.shape)\n","print(\"Salida test: \", y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qbvIffk205ET"},"source":["# Definimos la arquitectura\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n","\n","# Extracción caracteríisticas\n","input_layer = Input(shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3]))\n","conv1 = Conv2D(filters=8, kernel_size=(3,3), activation='relu')(input_layer)\n","max_pool1 = MaxPooling2D(pool_size=(2,2), strides=(2,2))(conv1)\n","\n","conv2 = Conv2D(filters=16, kernel_size=(3,3), activation='relu')(max_pool1)\n","max_pool2 = MaxPooling2D(pool_size=(2,2), strides=(2,2))(conv2)\n","\n","conv3 = Conv2D(filters=32, kernel_size=(3,3), activation='relu')(max_pool2)\n","max_pool3 = MaxPooling2D(pool_size=(2,2), strides=(2,2))(conv3)\n","\n","# Clasificación\n","flatten_layer = Flatten()(max_pool3) \n","hidden_layer = Dense(128, activation='relu')(flatten_layer)\n","output_layer = Dense(10, activation='softmax')(hidden_layer) # 10 salidas\n","\n","model = Model(inputs=input_layer, outputs=output_layer)\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XRMSljb63e95","executionInfo":{"status":"ok","timestamp":1621544194468,"user_tz":-120,"elapsed":524,"user":{"displayName":"Ana Jiménez Pastor","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgrLiw2d84vMG3Q3k5fXBnRwT77yp2o-XE46Ej8SA=s64","userId":"00856032343269336878"}}},"source":["# Compilamos el modelo\n","model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n","              metrics=[\"accuracy\"])"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"a323yG_v416E"},"source":["# Entenamos el modelo\n","history = model.fit(X_train, y_train_cod, epochs=20, batch_size=128,\n","                    validation_data=(X_val, y_val))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xqgaRFYQ5L65"},"source":["# Visualizamos la precisión\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Precisión modelo')\n","plt.ylabel('Precisión')\n","plt.xlabel('Época')\n","plt.ylim(0,1)\n","plt.legend(['Entrenamiento', 'Validación'], loc=\"lower right\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pE21Nl-m8Ne3"},"source":["# Evaluamos modelos\n","metrics = model.evaluate(X_test, y_test, verbose=0)\n","print('Precisión test: ', metrics[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"glf-Vf958DtK"},"source":["import numpy as np\n","\n","# Obtenemos predicciones \n","prediccion = model.predict(X_test)\n","# Cogemos la clase con mayor probabilidad\n","prediccion = np.argmax(prediccion, axis=1)\n","\n","# y_test lo tenemos en codificación OneHot, por lo que lo convertimos a clases\n","y_test_clases = np.argmax(y_test, axis=1)\n","\n","# Buscamos los índces de las imágenes corretamente e incorrectamente clasificadas\n","correct_index = np.nonzero(prediccion == y_test_clases)[0]\n","incorrect_index = np.nonzero(prediccion != y_test_clases)[0]\n","\n","# Mostramos 9 imágenes correctamente clasifcadas\n","plt.rcParams['figure.figsize'] = (10, 10)\n","for i, correct in enumerate(correct_index[:9]):\n","  plt.subplot(3, 3, i+1)\n","  plt.imshow(X_test[correct])\n","  plt.title(f'R: {classes[int(y_test_clases[correct])]}, P: {classes[int(prediccion[correct])]}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lm0QdZ-S8cWD"},"source":["# Mostramos 9 imágenes incorrectamente clasificadas\n","for i, incorrect in enumerate(incorrect_index[:9]):\n","  plt.subplot(3, 3, i+1)\n","  plt.imshow(X_test[incorrect])\n","  plt.title(f'R: {classes[int(y_test_clases[incorrect])]}, P: {classes[int(prediccion[incorrect])]}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BPoYU58y81Wz"},"source":["# Ejercicio 1: Entramiento durante más épocas\n","Vamos a entrenar el modelo durante 50 épocas\n","\n","NOTA: Para cada ejercicio genera una modelo diferente (con distinto nombre)"]},{"cell_type":"code","metadata":{"id":"rAZK27g16PTp"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xTnvPeu3-E5b"},"source":["# Ejercicio 2: Doblamos el número de filtros en cada capa convolucional\n","\n","Vamos a aumentar la complejidad de la arquitectura. Primero vamos a doblar el número de filtros de cada capa convolucional:\n","- conv1: 8 -> 16\n","- conv2: 16 -> 32\n","- conv3: 32 -> 64\n","\n","Para estas pruebas vuelve a poner el número de épocas a 20 para que los experimentos vayan más rápido."]},{"cell_type":"code","metadata":{"id":"aLRFuEyt9zD3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RwUSmrZx-0vt"},"source":["# Ejercicio 3. Aumentamos el número de capas convolucionales.\n","Tomando como partida el modelo inicial, donde ahora tenemos una capa convolucional añade otra justo antes con el mismo número de filtros y padding \"same\" (tamaño de salida igual al tamaño de entrada)."]},{"cell_type":"code","metadata":{"id":"FMiOpdGs-ii4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vhKRw4otApsc"},"source":["# Ejercicio 4: Aumentamos el tamaño de batch del modelo inicial\n","Vamos a cambiar el tamaño de batch a 250."]},{"cell_type":"code","metadata":{"id":"q-oTpmimASIG"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qf8OHJ_yEAj8"},"source":["Peores resultados con mayor tamaño de batch!!\n","\n","Puede parecer intuitivo que si ampliamos el tamaño de batch los resultados serán mejores ya que el modelo \"ve\" más imágenes de una. Pero esto no siempre es así. Hay que seleccionar el tamaño de batch con cuidado."]},{"cell_type":"markdown","metadata":{"id":"xMlOSSZPKX7o"},"source":["# Aumento de datos"]},{"cell_type":"code","metadata":{"id":"fqxT9M39KkdW","executionInfo":{"status":"ok","timestamp":1621544994355,"user_tz":-120,"elapsed":1313,"user":{"displayName":"Ana Jiménez Pastor","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgrLiw2d84vMG3Q3k5fXBnRwT77yp2o-XE46Ej8SA=s64","userId":"00856032343269336878"}}},"source":["# Definimos generador de datos\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","train_generator = ImageDataGenerator(\n","                                    rotation_range=5, \n","                                    horizontal_flip=True,\n","                                    zoom_range=.3)\n","\n","train_generator.fit(X_train)"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"gmHna-3kLkwr"},"source":["# Visualizamos imágenes aumentadas\n","augmented_images, _ = next( train_generator.flow(X_train, y_train_cod, batch_size=9))\n","plt.rcParams['figure.figsize'] = (10, 10)\n","for i in range(9):\n","  plt.subplot(3, 3, i+1)\n","  plt.imshow(augmented_images[i, :, :, :])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L4gD6ynDL1aK"},"source":["# Definimos arquitectura\n","input_layer = Input(shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3]))\n","conv1 = Conv2D(filters=8, kernel_size=(3,3), activation='relu')(input_layer)\n","max_pool1 = MaxPooling2D(pool_size=(2,2), strides=(2,2))(conv1)\n","\n","conv2 = Conv2D(filters=16, kernel_size=(3,3), activation='relu')(max_pool1)\n","max_pool2 = MaxPooling2D(pool_size=(2,2), strides=(2,2))(conv2)\n","\n","conv3 = Conv2D(filters=32, kernel_size=(3,3), activation='relu')(max_pool2)\n","max_pool3 = MaxPooling2D(pool_size=(2,2), strides=(2,2))(conv3)\n","\n","flatten_layer = Flatten()(max_pool3) \n","hidden_layer = Dense(128, activation='relu')(flatten_layer)\n","output_layer = Dense(10, activation='softmax')(hidden_layer) # 10 salidas\n","\n","model_6 = Model(inputs=input_layer, outputs=output_layer)\n","\n","# Compilamos el modelo\n","model_6.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n","                metrics=[\"accuracy\"])\n","# Entrenamos\n","\n","history_6 = model_6.fit_generator(train_generator.flow(X_train, y_train_cod, batch_size=128),\n","                                            epochs=20,\n","                                            steps_per_epoch=X_train.shape[0] // 128,\n","                                            validation_data=(X_val, y_val))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_O5DnUqHOql"},"source":["# Visualizamos la precisión\n","plt.plot(history_6.history['accuracy'])\n","plt.plot(history_6.history['val_accuracy'])\n","plt.title('Precisión modelo')\n","plt.ylabel('Precisión')\n","plt.xlabel('Época')\n","plt.ylim(0,1)\n","plt.legend(['Entrenamiento', 'Validación'], loc=\"lower right\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8UuDqSsUHTnh"},"source":["# Comparamos métricas entrenamento/validación con modelo inicial\n","print('Precisión modelo inicial: ')\n","print(' - Entrenamiento: ', history.history['accuracy'][-1])\n","print(' - Validación: ', history.history['val_accuracy'][-1])\n","print('Precisión modelo con aumento de datos: ')\n","print(' - Entrenamiento: ', history_6.history['accuracy'][-1])\n","print(' - Validación: ', history_6.history['val_accuracy'][-1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EtOg83WyHcuV"},"source":["# Comparamos las métricas en validación con el modelo inicia,\n","plt.plot(history.history['val_accuracy'])\n","plt.plot(history_6.history['val_accuracy'])\n","plt.title('Precisión modelo (Validación)')\n","plt.ylabel('Precisión')\n","plt.xlabel('Época')\n","plt.ylim(0,1)\n","plt.legend(['Modelo inicial', 'Modelo con aumento de datos'], loc=\"lower right\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s2Q__K4XHluE"},"source":["# Evaluamos sobre el conjunto de test y comparamos con modelo inicial\n","metrics_6 = model_6.evaluate(X_test, y_test, verbose=0)\n","print('Precisión modelo inicial (test): ', metrics[1])\n","print('Precisión modelo con aumento de datos (test): ', metrics_6[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fCpLSioCHsfQ"},"source":["Tras aplicar aumento de datos observamos una pequeña mejora de la precisión.\n","\n","Investiga el método de DataGenerator y prueba con más transformaciones. Incluso es posible aplicar aumento de datos al set de validación!"]},{"cell_type":"markdown","metadata":{"id":"wECwFgTIFgl8"},"source":["# Transferencia de conocimiento\n","\n","En este apartado vamos a aplicar tranferencia de conocimineto empleando un modelo pre-entrenado en la base de datos de Imagenet. Concretamente emplearemos el modelo ResNet50."]},{"cell_type":"code","metadata":{"id":"FAoarBqMOwBT"},"source":["from tensorflow.keras.applications.resnet50 import preprocess_input\n","import numpy as np\n","\n","# Volvemos al rango inicial de la imágenes [0-255] ya que se van a preprocesar de acuerdo a las necesidades de la red.\n","X_train *= 255\n","X_val *= 255\n","X_test *= 255\n","print(\"Rango imágenes: [\", X_train.min(), ', ', X_train.max(),']')\n","\n","# Preprocesamos los datos para que se ajusten a las necesidades del modelo selecccionado\n","X_train_prep = preprocess_input(X_train)\n","X_val_prep = preprocess_input(X_val)\n","X_test_prep = preprocess_input(X_test)\n","\n","print('Tamaño imágenes entrenamiento: ', X_train_prep.shape)\n","print('Valor mínimo imágenes entrenamiento: ', X_train_prep.min())\n","print('Valor máximo imágenes entrenamiento: ', X_train_prep.max())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QjZRDUwrDX3z"},"source":["from keras.applications import ResNet50\n","\n","# Cogemos modelo base a utilizar (ResNet50).\n","# Con include_top=False le estamos indicando que no queremos incluir la última capa (FC clasificación final)\n","# Con weights='imagenet' le estamos indicando que queremos inicializar los pesos con los que se obtuvieron en el entrenamiento con la BBDD de Imagenet.\n","base_model = ResNet50(include_top=False,weights='imagenet', input_shape=(32,32,3))\n","base_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mIFyFCXyI0SB"},"source":["No vamos a usar todas las capas de ResNet ya que podemos ver que muchos de los bloques acaban teniendo un tamaño de imagen de 1x1. Esto es debido a que las imágenes de CIFAR son mucho más pequeñas (32x32) que las de ImageNet (224x224).\n","\n","Vamos a coger hasta la capa \"conv3_block4_out\", que tiene un tamaño de imagen de 2x2 y vamos a congelar todas las capas. Es decir, durante el entrenamiento, los pesos de estas capas no se van a alterar, vamos a dejar los obtenidos en el entrenamiento con la base de datos de ImageNet. \n","A estas capas les vamos a incluir, a la salida, una fase de clasificación, que será aquello que entrenaremos."]},{"cell_type":"code","metadata":{"id":"kvD0-0y4H8jD"},"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Flatten, Dense, BatchNormalization, GlobalAveragePooling2D, Dropout\n","\n","# Congelamos todas las capas del clasificador VGG\n","\n","# Cogemos la salida de la capa \"conv3_block4_out\"\n","#x = base_model.output\n","x = base_model.get_layer('conv3_block4_out').output\n","\n","# Añadimos capas de clasificación\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(128, activation='relu')(x)\n","x = Dropout(0.5)(x)\n","output_layer = Dense(10, activation='softmax')(x)\n","model_resnet = Model(inputs=base_model.input, outputs=output_layer)\n","\n","for layer in base_model.layers:\n","     layer.trainable = False\n","\n","model_resnet.summary()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FpbVLKNoF3Ln"},"source":["Fijaros en el número de parámetros entrenables!! Todos los de ResNet no se van a alterar, únicamente los que hemos añadido con la capa totalmente conectada."]},{"cell_type":"code","metadata":{"id":"ghwUj4K3IlZf","executionInfo":{"status":"ok","timestamp":1621545603950,"user_tz":-120,"elapsed":514,"user":{"displayName":"Ana Jiménez Pastor","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgrLiw2d84vMG3Q3k5fXBnRwT77yp2o-XE46Ej8SA=s64","userId":"00856032343269336878"}}},"source":["# Compilamos el modelo\n","model_resnet.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n","                  metrics=[\"accuracy\"])"],"execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"id":"G2o7w5MUI8vp"},"source":["# Entenamos el modelo\n","history_resnet = model_resnet.fit(X_train_prep, y_train_cod, epochs=20, \n","                                  batch_size=128,\n","                                  validation_data=(X_val_prep, y_val))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bdnaC9TOJHMv"},"source":["# Visualizamos la precisión\n","import matplotlib.pyplot as plt\n","plt.plot(history_resnet.history['accuracy'])\n","plt.plot(history_resnet.history['val_accuracy'])\n","plt.title('Precisión modelo')\n","plt.ylabel('Precisión')\n","plt.xlabel('Época')\n","plt.legend(['Entrenamiento', 'Validación'], loc=\"upper left\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-lNlZ1sufEk"},"source":["# Comparativa con modelo inicial\n","plt.plot(history.history['val_accuracy'])\n","plt.plot(history_resnet.history['val_accuracy'])\n","plt.title('Precisión modelo (validación')\n","plt.ylabel('Precisión')\n","plt.xlabel('Época')\n","plt.legend(['CNN propia', 'Transfer learning (ResNet)'], loc=\"lower right\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ze7VndPvMGuF"},"source":["# Evaluamos sobre conjunto de test\n","metrics_resnet = model_resnet.evaluate(X_test_prep, y_test, verbose=0)\n","print(\"Precision test CNN propia: \", metrics[1])\n","print(\"Precision test transferencia conocimiento: \", metrics_resnet[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5UviWxLm4R68"},"source":["Podemos observar un aumento en la precisión.\n","\n","Tras haber realizado una transferencia de conocimiento congelando las capas del modelo base, se puede hace un ajuste fino del modelo adaptando los pesos de toda la red empleando una tasa de aprendizaje baja durante pocas épocas (para evitar el sobreajuste)."]},{"cell_type":"code","metadata":{"id":"tUqj-irTRTi2"},"source":["from tensorflow.keras.optimizers import Adam\n","# Descongelamos las capas del modelo base\n","for layer in base_model.layers:\n","\tlayer.trainable = True\n","\n","# Compilamos el modelo con una tasa de aprendizaje más baja\n","model_resnet.compile(optimizer=Adam(1e-5),\n","              loss= 'categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Entrenamos modelo durante unas pocas épocas.\n","history_resnet_fine = model_resnet.fit(X_train_prep, y_train_cod, epochs=10, \n","                                       batch_size=128,\n","                                       validation_data=(X_val_prep, y_val))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ASgePpwISWW0"},"source":["# Evaluamos sobre conjunto de test\n","metrics_resnet_2 = model_resnet.evaluate(X_test_prep, y_test, verbose=0)\n","print(\"Precision test CNN propia: \", metrics[1])\n","print(\"Precision test transferencia conocimiento: \", metrics_resnet_2[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N73L8IuyMk2M"},"source":["# Obtenemos predicciones \n","prediccion = model_resnet.predict(X_test_prep)\n","# Cogemos la clase con mayor probabilidad\n","prediccion = np.argmax(prediccion, axis=1)\n","\n","# y_test lo tenemos en codificación OneHot, por lo que lo convertimos a clases\n","y_test_clases = np.argmax(y_test, axis=1)\n","\n","# Buscamos los índces de las imágenes corretamente e incorrectamente clasificadas\n","correct_index = np.nonzero(prediccion == y_test_clases)[0]\n","incorrect_index = np.nonzero(prediccion != y_test_clases)[0]\n","\n","# Mostramos 9 imágenes correctamente clasificadas\n","plt.rcParams['figure.figsize'] = (10, 10)\n","for i, correct in enumerate(correct_index[:9]):\n","  # Convertimos imagen al rango [0, 1] para visualizarla\n","  image = X_test[correct]\n","  for j in range(3):\n","    image_ch = image[:, :, j]\n","    image[:, :, j] = (image_ch - image_ch.min()) / (image_ch.max() - image_ch.min())\n","  plt.subplot(3, 3, i+1)\n","  plt.imshow(image)\n","  plt.title(f'R: {classes[int(y_test_clases[correct])]}, P: {classes[int(prediccion[correct])]}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5DzVb8uqqWh"},"source":["# Mostramos 9 imágenes incorrectamente clasificadas\n","plt.rcParams['figure.figsize'] = (10, 10)\n","for i, incorrect in enumerate(incorrect_index[:9]):\n","  # Convertimos imagen al rango [0, 1] para visualizarla\n","  image = X_test[incorrect]\n","  for j in range(3):\n","    image_ch = image[:, :, j]\n","    image[:, :, j] = (image_ch - image_ch.min()) / (image_ch.max() - image_ch.min())\n","  plt.subplot(3, 3, i+1)\n","  plt.imshow(image)\n","  plt.title(f'R: {classes[int(y_test_clases[incorrect])]}, P: {classes[int(prediccion[incorrect])]}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kOt2LjGetc22"},"source":[""],"execution_count":null,"outputs":[]}]}